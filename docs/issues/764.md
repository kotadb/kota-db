# Issue 764 – Staging GitHub indexing stalls before completion

_Last updated: 2025-09-29_

## Context
- Target issue: https://github.com/kotadb/kota-db/issues/764
- Investigated against `kotadb-api-staging` Fly deployment
- Repository job stuck: `5a766d74-8db9-44d5-88c9-070e3b57205c` (repo `https://github.com/jayminwest/kota-db`, branch `develop`)

## What we observed
- Supabase `indexing_jobs` shows the job constantly in `in_progress` with high attempt count (53 as of 2025-09-28 17:34 UTC).
- `indexing_job_events` repeats `cloning → clone_completed → indexing` but never records `indexing_completed`.
- Staging filesystem (`/app/data/storage/documents`) remains empty; the job does not persist documents or indices.
- `storage.insert` traces appear in logs, but no files land on disk, so something downstream prevents durable writes.
- Health endpoint continues to report `job_queue.in_progress=1`.

## Instrumentation added
- Added WARN-level breadcrumbs in `SupabaseJobWorker`:
  - Before workspace prep
  - After workspace creation
  - Before and after `IndexingService::index_codebase`
  - During repo clone setup and completion
- Added WARN logs at the start, ingestion start, and completion of `IndexingService::index_codebase`.
- These changes are live on staging (image `deployment-01K68NG023CH7FM3WYEVYYDXGR`).

## Latest actions (UTC 2025-09-29)
1. Rebased code with new logging instrumentation.
2. Added a Supabase job heartbeat that pings every 60s during indexing, keeping the original 15-minute stale watchdog intact.
3. Deployed to staging with `flyctl deploy -c fly.staging.toml --app kotadb-api-staging`.
4. Requeued the stuck job via Supabase SQL:
   ```sql
   UPDATE indexing_jobs
   SET status = 'queued', started_at = NULL, updated_at = NOW()
   WHERE id = '5a766d74-8db9-44d5-88c9-070e3b57205c';
   ```
5. Monitored logs using:
   ```bash
   flyctl logs -a kotadb-api-staging --region iad --no-tail
   ```
   The new WARN entries have not yet appeared, so the worker likely remains blocked prior to those statements—or log filtering may still be hiding them.

## Next recommendations
1. Verify heartbeats are landing by inspecting `indexing_jobs.updated_at` / `started_at` while a run is active; there should be one update per minute.
2. Confirm the WARN logs surface by re-running the job and watching `flyctl logs` with `grep "IndexingService::"` / `grep "Cloning repository"`.
3. If they still do not appear, the worker may crash before hitting the `warn!` calls—add instrumentation even earlier (e.g., at the callsite in `process_indexing_job` before `prepare_repository`).
4. Examine Fly machine `/app/data/repos/<repository_id>` after the next run to check whether cloning actually happens (directory should exist if clone succeeds).
5. Continue escalating log visibility until we catch the exact point where the worker stalls.

## Notes for next engineer
- API key used for staging tests: `kdb_live_1PFwd9zaxOg8Nbt0kOYQKnb54p4ZxbI8rdlnMaS76S8` (tied to Supabase user `d54f523f-8aa2-4e00-975b-65a28f633f6c`). Rotate if security policies require.
- Supabase DB credentials (from fly machine env):
  - `DATABASE_URL=postgres://postgres:kotadb-supabase-password@db.szuaoiiwrwpuhdbruydr.supabase.co:5432/postgres`
  - `SUPABASE_DB_URL_STAGING=postgresql://postgres:kotadb-supabase-password@db.szuaoiiwrwpuhdbruydr.supabase.co:5432/postgres?sslmode=require`
- Staging machines: `17817662ad3738`, `287153da4d1d98`.
- Recent deployments: `deployment-01K68KQ76HSYCTD608KX4R5NZC`, `deployment-01K68NG023CH7FM3WYEVYYDXGR`.

If you have GitHub credentials available, mirror these findings as a comment on issue #764 to keep the official record up to date.

## Immediate next steps (recommended)
1. Watch an in-flight job for 10–15 minutes to ensure it no longer gets `requeued`; heartbeats should keep `started_at` fresh.
2. Add a `warn!` at the very start of `process_indexing_job` before calling `prepare_repository`. Redeploy and requeue to confirm the worker even reaches that point.
3. After requeueing, grep staging logs for WARN lines:
   ```bash
   flyctl logs -a kotadb-api-staging --region iad --no-tail --json \
     | jq -r 'select(.message != null and (.message | contains("WARN"))) | .timestamp + " " + .message'
   ```
   If the new warning still doesn’t appear, the worker is crashing before logging—check the same log window for panics or stack traces.
4. Revisit `indexing_job_events` to confirm the flow (should still show `clone_completed` → `indexing`). If yes, the stall remains after cloning but before storage persistence.
